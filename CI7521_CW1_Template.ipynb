{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHKla251wzqs"
      },
      "source": [
        "# Coursework 1 Group (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEpvMZ72w_sa"
      },
      "source": [
        "Complete the following information\n",
        "\n",
        "Group number: ...\n",
        "\n",
        "Student names and k-numbers:\n",
        "1. Bhumika Reddy Kurubarahalli Sudharshan (K2463984)\n",
        "2. Pragati Priya (K2461276)\n",
        "3. Reghuram Karunamurthi (K2446531)\n",
        "4. Shreenidhi Dayanand Shetty (K2455943)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bB4y2t38MoG"
      },
      "source": [
        "# Load modules (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwn_9siuy7oS"
      },
      "outputs": [],
      "source": [
        "# add code for loading modules\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.metrics import (confusion_matrix, balanced_accuracy_score, roc_auc_score, accuracy_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytKXenzY9H2X"
      },
      "source": [
        "# Load data (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cpm05eF9WU2"
      },
      "outputs": [],
      "source": [
        "# add code for loading data\n",
        "\n",
        "digits = load_digits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds2nwgc4-6fW"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulMsXj9C_URf"
      },
      "source": [
        "## Classification methods used (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2iaMuT8IEKZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sD3KL9J_cMw"
      },
      "source": [
        "Mention the classification methods used below. You should not describe them, but if they have not been discussed in the class, you should cite the source:\n",
        "\n",
        "\n",
        "1. Decision Tree Classifier\n",
        "2. Linear discriminant analysis (Kaggle,2024) (Ashraf, K. 2024)\n",
        "3. Logistic Regression (Madecraft and Galarnyk M., 2020) (Geron, 2017)\n",
        "4. Gaussian Naive Bayes (Analytics Vidhya, 2025) (Medium,2020)\n",
        "5. Support Vector Machine\n",
        "6. K-nearest neighbors (KNN) (Kaggle,2022) (Singh, B 2022)\n",
        "7. Random Forest (Metric Coders 2024)\n",
        "8. Quadratic Discriminant Analysis (Scikit Learn 2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azwz2930HKu7"
      },
      "source": [
        "## Training (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBCgkDaWHKv1"
      },
      "outputs": [],
      "source": [
        "#add code for training\n",
        "\n",
        "# Decision Tree\n",
        "\n",
        "#selecting features and target\n",
        "X_dt = digits.data\n",
        "Y_dt = digits.target\n",
        "\n",
        "# splitting training and testing data \n",
        "X_dt_train, X_dt_test, Y_dt_train, Y_dt_test = train_test_split(X_dt, Y_dt, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# standardize the features \n",
        "scaler = StandardScaler()\n",
        "X_dt_train = scaler.fit_transform(X_dt_train)\n",
        "X_dt_test = scaler.transform(X_dt_test)\n",
        "\n",
        "# defining the model\n",
        "model_dt = DecisionTreeClassifier() # (Scikit Learn 2025)\n",
        "\n",
        "# fit the model\n",
        "model_dt.fit(X_dt_train, Y_dt_train)\n",
        "\n",
        "\n",
        "\n",
        "# Linear discriminant analysis\n",
        "\n",
        "#selecting features and target\n",
        "X_lda = digits.data\n",
        "Y_lda = digits.target\n",
        "Y__lda_bin = label_binarize(Y_lda, classes=np.unique(Y_lda)) \n",
        "\n",
        "# splitting training and testing data (80% train and 20% test)\n",
        "X__lda_train, X_lda_test, Y_lda_train, Y_lda_test, Y_lda_bin_train, Y_lda_bin_test = train_test_split(X_lda, Y_lda, Y__lda_bin, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# standardizing the features \n",
        "scaler = StandardScaler()\n",
        "X__lda_train = scaler.fit_transform(X__lda_train)\n",
        "X_lda_test = scaler.transform(X_lda_test)\n",
        "\n",
        "# defining the LDA model\n",
        "model_lda = LinearDiscriminantAnalysis() #(Kaggle,2024) (Ashraf, K. 2024)\n",
        "\n",
        "# fit the model\n",
        "model_lda.fit(X__lda_train, X_lda_test)\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "#checking for dataset's properties\n",
        "df_lr = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
        "dir(digits)\n",
        "\n",
        "#selecting features and target\n",
        "X_lr = df_lr.loc[:, digits.feature_names].values\n",
        "Y_lr = digits.target\n",
        "\n",
        "#spliting traing and testing data\n",
        "X_lr_train, X_lr_test, Y_lr_train, Y_lr_test = train_test_split(X_lr, Y_lr, test_size=0.2, random_state=72)\n",
        "\n",
        "#scaling the data for max number of iterations\n",
        "scaler_lr = StandardScaler()\n",
        "X_lr_train = scaler_lr.fit_transform(X_lr_train)\n",
        "X_lr_test = scaler_lr.transform(X_lr_test)\n",
        "\n",
        "#fit the model\n",
        "model_lr = LogisticRegression(max_iter=1000) #(Madecraft and Galarnyk M., 2020)(Geron, 2017)\n",
        "model_lr.fit(X_lr_train, Y_lr_train)\n",
        "\n",
        "\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "\n",
        "#checking for dataset's properties\n",
        "df_gnb = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
        "\n",
        "#selecting features and target\n",
        "X_gnb = df_gnb.loc[:, digits.feature_names].values\n",
        "Y_gnb = digits.target\n",
        "\n",
        "#spliting traing and testing data\n",
        "X_gnb_train, X_gnb_test, Y_gnb_train, Y_gnb_test = train_test_split(X_gnb,Y_gnb, test_size=0.2, random_state=72)\n",
        "\n",
        "#fit the model\n",
        "model_gnb = GaussianNB() #(Scikit, 2025)\n",
        "model_gnb.fit(X_gnb_train, Y_gnb_train)\n",
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "\n",
        "# selecting features and target\n",
        "X_svm = digits.data\n",
        "Y_svm = digits.target\n",
        "\n",
        "# splting traning and testing data (80% train and 20% test)\n",
        "X_svm_train, X_svm_test, Y_svm_train, Y_svm_test = train_test_split(X_svm, Y_svm, test_size=0.2, random_state=72)\n",
        "\n",
        "# defining the model\n",
        "model_svm = svm.SVC(gamma=0.001, probability=True) # (Scikit,2025)\n",
        "\n",
        "# fit the model\n",
        "model_svm.fit(X_svm_train, Y_svm_train)\n",
        "\n",
        "\n",
        "\n",
        "# K-nearest neighbors (KNN)\n",
        "\n",
        "# Selecting features and target\n",
        "X_knn = digits.data\n",
        "Y_knn = digits.target\n",
        "\n",
        "# Splting traning and testing data (80% train and 20% test)\n",
        "X_knn_train, X_knn_test, Y_knn_train, Y_knn_test = train_test_split(X_knn, Y_knn, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# Standardize the features since KNN is sensitive when scaling\n",
        "scaler = StandardScaler()\n",
        "X_knn_train = scaler.fit_transform(X_knn_train)\n",
        "X_knn_test = scaler.transform(X_knn_test)\n",
        "\n",
        "# Defining the model\n",
        "k = 5\n",
        "model_knn = KNeighborsClassifier(n_neighbors=k) #(Kaggle,2022) (Singh, B 2022) \n",
        "\n",
        "# Fit the model\n",
        "model_knn.fit(X_knn_train, Y_knn_train)\n",
        "\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "# Selecting features and target\n",
        "X_rf = digits.data\n",
        "Y_rf = digits.target\n",
        "\n",
        "# Splitting training and testing data (80% train and 20% test)\n",
        "X_rf_train, X_rf_test, Y_rf_train, Y_rf_test = train_test_split(X_rf, Y_rf, test_size=0.2, random_state=72)\n",
        "\n",
        "# defining the model\n",
        "model_rf = RandomForestClassifier(random_state=72) # (Metric Coders 2024)\n",
        "\n",
        "# train the model \n",
        "model_rf.fit(X_rf_train, Y_rf_train)\n",
        "\n",
        "\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "\n",
        "# Selecting features and target\n",
        "X_qda = digits.data\n",
        "Y_qda = digits.target\n",
        "\n",
        "# Split the data\n",
        "X_qda_train, X_qda_test, Y_qda_train, Y_qda_test = train_test_split(X_qda, Y_qda, test_size=0.2, random_state=72)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_qda_train = scaler.fit_transform(X_qda_train)\n",
        "X_qda_test = scaler.transform(X_qda_test)\n",
        "\n",
        "# Defining the QDA model\n",
        "model_qda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "\n",
        "# Fit the model\n",
        "model_qda.fit(X_qda_train, Y_qda_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnXrUB6yHKv5"
      },
      "source": [
        "## Evaluation (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nUIhvoYgHKv7"
      },
      "outputs": [],
      "source": [
        "#add code for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow5hzq1vItvJ"
      },
      "source": [
        "# References (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d4gWJO3I1KM"
      },
      "source": [
        "List any references you may have used in your document before, using one of the established referencing system (e.g. IEEE, Harvard, etc).\n",
        "\n",
        "# Reference List\n",
        "\n",
        "Pragati\n",
        "\n",
        "1. Gao, Z. (2020) Everything you need to know about Naive Bayes. Available at [https://medium.com/analytics-vidhya/everything-you-need-to-know-about-na%C3%AFve-bayes-9a97cff1cba3](https://medium.com/analytics-vidhya/everything-you-need-to-know-about-na%C3%AFve-bayes-9a97cff1cba3) (Access 21 February 2025\\)  \n",
        "2. Geron A. (2017) Hands-On Machine Learning with Scikit-Learn and TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems. United States of Americas: O’Reilly Media, Inc.\t  \n",
        "3. Madecraft and Galarnyk M. (2020) Logistic Regression using scikit-learn. Linkedin Learning. Available at [https://www.linkedin.com/learning/machine-learning-with-scikit-learn/logistic-regression-using-scikit-learn?u=56743409](https://www.linkedin.com/learning/machine-learning-with-scikit-learn/logistic-regression-using-scikit-learn?u=56743409) (Accessed 11 February 2025\\)  \n",
        "4. Ray, S. (2025) Naive Bayes Classifier Explained With Practical Problems. Available at [https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/) (Accessed: 21 February 2025\\)  \n",
        "5. Scikit Learn (2025) 1.9. Naive Bayes. Available at [https://scikit-learn.org/stable/modules/naive\\_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html) (Accessed: 21 February 2025\\)  \n",
        "6. Scikit Learn (2025) 3.2. Tuning the hyper-parameters of an estimator. Available at [https://scikit-learn.org/stable/modules/grid\\_search.html](https://scikit-learn.org/stable/modules/grid_search.html) (Accessed 23 February 2025\\)  \n",
        "7. Scikit Learn (2025) 6.3. Preprocessing Data. Available at [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html) (Accessed 12 february 2025\\)  \n",
        "8. Scikit Learn (2025) balanced\\_accuary\\_score. Available at  [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\\_accuracy\\_score.html\\#sklearn.metrics.balanced\\_accuracy\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (Accessed 12 February 2025\\)  \n",
        "9. Scikit Learn (2025) f1\\_score. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1\\_score.html\\#sklearn.metrics.f1\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) (Accessed 12 February 2025\\)  \n",
        "10. Scikit Learn (2025) Gaussian NB. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.naive\\_bayes.GaussianNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) (Accessed 24 February 2025\\)  \n",
        "11. Scikit Learn (2025) Logistic Regression. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.linear\\_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (Accessed 11 February 2025\\)  \n",
        "12. Scikit Learn (2025) roc\\_acc\\_score. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\_auc\\_score.html\\#sklearn.metrics.roc\\_auc\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) (Accessed 22 February 2025\\)\n",
        "\n",
        "Reghuram\n",
        "\n",
        "13. Singh, B (2022) Knn classification on Digits dataset. Available at [https://www.kaggle.com/code/bandhansingh/knn-classification-on-digits-dataset] (https://www.kaggle.com/code/bandhansingh/knn-classification-on-digits-dataset)\n",
        "14. Agrawal, S. (2023) Hyperparameter Tuning of KNN Classifier. Available at [https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7] (https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7)\n",
        "15. Scikit Learn (2025) Recognizing hand-written digits. Available at [https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html] (https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)\n",
        "16. Scikit Learn (2025) 3.2. Tuning the hyper-parameters of an estimator. Available at [https://scikit-learn.org/stable/modules/grid\\_search.html](https://scikit-learn.org/stable/modules/grid_search.html)\n",
        "17. Scikit Learn (2025) balanced\\_accuary\\_score. Available at  [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\\_accuracy\\_score.html\\#sklearn.metrics.balanced\\_accuracy\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (Accessed 12 February 2025\\) \n",
        "18. Scikit Learn (2025) roc\\_acc\\_score. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\_auc\\_score.html\\#sklearn.metrics.roc\\_auc\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) (Accessed 22 February 2025\\)\n",
        "19. Scikit Learn (2025) Confusion Matrix. [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html] (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "\n",
        "\n",
        "Bhumika\n",
        "\n",
        "20. Ashraf, K. (2024) Digit Classification Using PCA and LDA Techniques. Available at [https://www.kaggle.com/code/khaledashrafm3wad/digit-classification-using-pca-and-lda-techniques] (https://www.kaggle.com/code/khaledashrafm3wad/digit-classification-using-pca-and-lda-techniques)\n",
        "21. Hernández, J. (2019) Tactic 03. Hyperparameter optimization. LDA. Available at [https://www.kaggle.com/code/juanmah/tactic-03-hyperparameter-optimization-lda] (https://www.kaggle.com/code/juanmah/tactic-03-hyperparameter-optimization-lda)\n",
        "22. Scikit Learn (2025) 1.10.1. Classification. Available at [https://scikit-learn.org/stable/modules/tree.html] (https://scikit-learn.org/stable/modules/tree.html)\n",
        "23. Brownlee, J (2020) Tune Hyperparameters for Classification Machine Learning Algorithms. Available at [https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/] (https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/)\n",
        "24. Scikit Learn (2025) balanced\\_accuary\\_score. Available at  [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\\_accuracy\\_score.html\\#sklearn.metrics.balanced\\_accuracy\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (Accessed 12 February 2025\\) \n",
        "25. Scikit Learn (2025) roc\\_acc\\_score. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\_auc\\_score.html\\#sklearn.metrics.roc\\_auc\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) (Accessed 22 February 2025\\)\n",
        "26. Scikit Learn (2025) Confusion Matrix. [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html] (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "\n",
        "\n",
        "Shreenidhi\n",
        "\n",
        "27. Metric Coders (2024) Machine Learning using Random Forest Classifier. Available at [https://www.metriccoders.com/post/machine-learning-using-random-forest-classifier] (https://www.metriccoders.com/post/machine-learning-using-random-forest-classifier)\n",
        "28. GeeksForeeks (2025) Random Forest Hyperparameter Tuning in Python. Available at [https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/] (https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/)\n",
        "29. Scikit Learn (2025) Quadratic Discriminant Analysis (QDA). Available at https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html (Accessed 28 February 2025)\n",
        "30. Katsu (2019) Best parameter(s) for QDA. Available at [https://www.kaggle.com/code/code1110/best-parameter-s-for-qda] (https://www.kaggle.com/code/code1110/best-parameter-s-for-qda)\n",
        "31. Scikit Learn (2025) balanced\\_accuary\\_score. Available at  [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced\\_accuracy\\_score.html\\#sklearn.metrics.balanced\\_accuracy\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (Accessed 12 February 2025\\) \n",
        "32. Scikit Learn (2025) roc\\_acc\\_score. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\_auc\\_score.html\\#sklearn.metrics.roc\\_auc\\_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) (Accessed 22 February 2025\\)\n",
        "33. Scikit Learn (2025) Confusion Matrix. [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html] (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GHKla251wzqs",
        "1bB4y2t38MoG",
        "ytKXenzY9H2X",
        "ulMsXj9C_URf",
        "azwz2930HKu7",
        "YnXrUB6yHKv5",
        "Ow5hzq1vItvJ"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
