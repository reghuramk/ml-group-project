{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHKla251wzqs"
      },
      "source": [
        "# Coursework 1 Group (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEpvMZ72w_sa"
      },
      "source": [
        "Complete the following information\n",
        "\n",
        "Group number: CW1 Group 4\n",
        "\n",
        "Student names and k-numbers:\n",
        "1. Bhumika Reddy Kurubarahalli Sudharshan (K2463984)\n",
        "2. Pragati Priya (K2461276)\n",
        "3. Reghuram Karunamurthi (K2446531)\n",
        "4. Shreenidhi Dayanand Shetty (K2455943)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-i-iBhB252a"
      },
      "source": [
        "##Load Modules (code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rg-y6bd_t74B"
      },
      "outputs": [],
      "source": [
        "# Loading Modules\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Third-party libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (confusion_matrix, balanced_accuracy_score, f1_score, roc_auc_score, accuracy_score, roc_curve, auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AzHJUto29gz"
      },
      "source": [
        "##Load Data (code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nisyzoUguNNU"
      },
      "outputs": [],
      "source": [
        "# Loading Digits dataset\n",
        "\n",
        "digits = load_digits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQlX_g6F3DT-"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAo9qXm3FFF"
      },
      "source": [
        "##Classification methods used (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st46LvPr3KOE"
      },
      "source": [
        "Mention the classification methods used below. You should not describe them, but if they have not been discussed in the class, you should cite the source:\n",
        "\n",
        "\n",
        "1. Decision Tree Classifier\n",
        "2. Linear discriminant analysis (Kaggle,2024) (Ashraf, K. 2024)\n",
        "3. Logistic Regression (Madecraft and Galarnyk M., 2020) (Geron, 2017)\n",
        "4. Gaussian Naive Bayes (Analytics Vidhya, 2025) (Medium,2020)\n",
        "5. Support Vector Machine\n",
        "6. K-nearest neighbors (KNN) (Kaggle,2022) (Singh, B 2022)\n",
        "7. Random Forest (Metric Coders 2024)\n",
        "8. Quadratic Discriminant Analysis (Scikit Learn 2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPGP1FB3NcT"
      },
      "source": [
        "Training (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPFZhVN23QNu"
      },
      "source": [
        "Training Each model Separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmXHvuEvuWMI",
        "outputId": "2febca7c-ae78-4073-b891-518f56e6740e"
      },
      "outputs": [],
      "source": [
        "#add code for training\n",
        "\n",
        "# Decision Tree\n",
        "\n",
        "#selecting features and target\n",
        "X_dt = digits.data\n",
        "Y_dt = digits.target\n",
        "\n",
        "# splitting training and testing data\n",
        "X_dt_train, X_dt_test, Y_dt_train, Y_dt_test = train_test_split(X_dt, Y_dt, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_dt_train = scaler.fit_transform(X_dt_train)\n",
        "X_dt_test = scaler.transform(X_dt_test)\n",
        "\n",
        "# defining the model\n",
        "model_dt = DecisionTreeClassifier() # (Scikit Learn 2025)\n",
        "\n",
        "# fit the model\n",
        "model_dt.fit(X_dt_train, Y_dt_train)\n",
        "\n",
        "\n",
        "\n",
        "# Linear discriminant analysis\n",
        "\n",
        "#selecting features and target \n",
        "X_lda = digits.data\n",
        "Y_lda = digits.target\n",
        "Y__lda_bin = label_binarize(Y_lda, classes=np.unique(Y_lda))\n",
        "\n",
        "# splitting training and testing data (80% train and 20% test)\n",
        "X__lda_train, X_lda_test, Y_lda_train, Y_lda_test, Y_lda_bin_train, Y_lda_bin_test = train_test_split(X_lda, Y_lda, Y__lda_bin, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X__lda_train = scaler.fit_transform(X__lda_train)\n",
        "X_lda_test = scaler.transform(X_lda_test)\n",
        "\n",
        "# defining the LDA model\n",
        "model_lda = LinearDiscriminantAnalysis() #(Kaggle,2024) (Ashraf, K. 2024)\n",
        "\n",
        "# fit the model\n",
        "model_lda.fit(X__lda_train, Y_lda_train)\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "#checking for dataset's properties\n",
        "df_lr = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
        "dir(digits)\n",
        "\n",
        "#selecting features and target\n",
        "X_lr = df_lr.loc[:, digits.feature_names].values\n",
        "Y_lr = digits.target\n",
        "\n",
        "#spliting traing and testing data\n",
        "X_lr_train, X_lr_test, Y_lr_train, Y_lr_test = train_test_split(X_lr, Y_lr, test_size=0.2, random_state=72)\n",
        "\n",
        "#scaling the data for max number of iterations\n",
        "scaler_lr = StandardScaler()\n",
        "X_lr_train = scaler_lr.fit_transform(X_lr_train)\n",
        "X_lr_test = scaler_lr.transform(X_lr_test)\n",
        "\n",
        "#fit the model\n",
        "model_lr = LogisticRegression(max_iter=1000) #(Madecraft and Galarnyk M., 2020)(Geron, 2017)\n",
        "model_lr.fit(X_lr_train, Y_lr_train)\n",
        "\n",
        "\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "\n",
        "#checking for dataset's properties\n",
        "df_gnb = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
        "\n",
        "#selecting features and target\n",
        "X_gnb = df_gnb.loc[:, digits.feature_names].values\n",
        "Y_gnb = digits.target\n",
        "\n",
        "#spliting traing and testing data\n",
        "X_gnb_train, X_gnb_test, Y_gnb_train, Y_gnb_test = train_test_split(X_gnb,Y_gnb, test_size=0.2, random_state=72)\n",
        "\n",
        "#fit the model\n",
        "model_gnb = GaussianNB() #(Scikit, 2025)\n",
        "model_gnb.fit(X_gnb_train, Y_gnb_train)\n",
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "\n",
        "# selecting features and target\n",
        "X_svm = digits.data\n",
        "Y_svm = digits.target\n",
        "\n",
        "# splting traning and testing data (80% train and 20% test)\n",
        "X_svm_train, X_svm_test, Y_svm_train, Y_svm_test = train_test_split(X_svm, Y_svm, test_size=0.2, random_state=72)\n",
        "\n",
        "# defining the model\n",
        "model_svm = svm.SVC(gamma=0.001, probability=True) # (Scikit,2025)\n",
        "\n",
        "# fit the model\n",
        "model_svm.fit(X_svm_train, Y_svm_train)\n",
        "\n",
        "\n",
        "# K-nearest neighbors (KNN)\n",
        "\n",
        "# Selecting features and target\n",
        "X_knn = digits.data\n",
        "Y_knn = digits.target\n",
        "\n",
        "# Splting traning and testing data (80% train and 20% test)\n",
        "X_knn_train, X_knn_test, Y_knn_train, Y_knn_test = train_test_split(X_knn, Y_knn, test_size=0.2, random_state=72, shuffle=True)\n",
        "\n",
        "# Standardize the features since KNN is sensitive when scaling\n",
        "scaler = StandardScaler()\n",
        "X_knn_train = scaler.fit_transform(X_knn_train)\n",
        "X_knn_test = scaler.transform(X_knn_test)\n",
        "\n",
        "# Defining the model\n",
        "k = 5\n",
        "model_knn = KNeighborsClassifier(n_neighbors=k) #(Kaggle,2022) (Singh, B 2022)\n",
        "\n",
        "# Fit the model\n",
        "model_knn.fit(X_knn_train, Y_knn_train)\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "# Selecting features and target\n",
        "X_rf = digits.data\n",
        "Y_rf = digits.target\n",
        "\n",
        "# Splitting training and testing data (80% train and 20% test)\n",
        "X_rf_train, X_rf_test, Y_rf_train, Y_rf_test = train_test_split(X_rf, Y_rf, test_size=0.2, random_state=72)\n",
        "\n",
        "# defining the model\n",
        "model_rf = RandomForestClassifier(random_state=72) # (Metric Coders 2024)\n",
        "\n",
        "# train the model\n",
        "model_rf.fit(X_rf_train, Y_rf_train)\n",
        "\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "\n",
        "# Selecting features and target\n",
        "X_qda = digits.data\n",
        "Y_qda = digits.target\n",
        "\n",
        "# Split the data\n",
        "X_qda_train, X_qda_test, Y_qda_train, Y_qda_test = train_test_split(X_qda, Y_qda, test_size=0.2, random_state=72)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_qda_train = scaler.fit_transform(X_qda_train)\n",
        "X_qda_test = scaler.transform(X_qda_test)\n",
        "\n",
        "# Defining the QDA model\n",
        "model_qda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "\n",
        "# Fit the model\n",
        "model_qda.fit(X_qda_train, Y_qda_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bvyBxN3VM9"
      },
      "source": [
        "##Evaluation (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZI7Y8M43YHN"
      },
      "source": [
        "Evaluating each model with default parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MJDjimeluc-z",
        "outputId": "38e3b5b8-37eb-457b-a5cb-9d0180f2fdaa"
      },
      "outputs": [],
      "source": [
        "#add code for evaluation\n",
        "\n",
        "\n",
        "# Decision Tree\n",
        "\n",
        "# predictions\n",
        "pred_dt = model_dt.predict(X_dt_test)\n",
        "y_prob_dt = model_dt.predict_proba(X_dt_test)\n",
        "\n",
        "# evaluations\n",
        "bal_acc_dt = balanced_accuracy_score(Y_dt_test, pred_dt) #(Scikit,2025)\n",
        "roc_dt = roc_auc_score(Y_dt_test, y_prob_dt, multi_class=\"ovr\", average=\"macro\") #(Scikit,2025)\n",
        "\n",
        "print(f\"Decision Tree Balanced Accuracy: {bal_acc_dt:.4f}\")\n",
        "print(f\"Decision Tree ROC-AUC Score: {roc_dt:.4f}\")\n",
        "\n",
        "\n",
        "# Linear discriminant analysis\n",
        "\n",
        "# Predictions\n",
        "pred_lda = model_lda.predict(X_lda_test)\n",
        "y_prob_lda = model_lda.predict_proba(X_lda_test)\n",
        "\n",
        "# Evaluations\n",
        "bal_acc_lda = balanced_accuracy_score(Y_lda_test, pred_lda) #measuring balanced accuracy (Scikit,2025)\n",
        "roc_lda = roc_auc_score(Y_lda_test, y_prob_lda, multi_class=\"ovr\", average=\"macro\") #calculating ROC score (SciKit,2025)\n",
        "\n",
        "print(f\"\\nLDA Balanced Accuracy: {bal_acc_lda:.4f}\")\n",
        "print(f\"LDA ROC-AUC Score: {roc_lda:.4f}\")\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "# model prediction\n",
        "pred_lr = model_lr.predict(X_lr_test)\n",
        "\n",
        "# evaluation\n",
        "bal_acc_lr = balanced_accuracy_score(Y_lr_test, pred_lr) #measuring balanced accuracy (Scikit,2025)\n",
        "f1_check_lr = f1_score(Y_lr_test, pred_lr, average=\"weighted\") #measuring F1 score (Scikit,2025)\n",
        "roc_lr = roc_auc_score(Y_lr_test, model_lr.predict_proba(X_lr_test), multi_class=\"ovr\", average=\"macro\") #calculating ROC score (SCIKIT,2025)\n",
        "\n",
        "print(\"\\nLogistic Regression Balanced Accuracy:\", bal_acc_lr)\n",
        "print(\"Logistic Regression F1 Score:\", f1_check_lr)\n",
        "print(\"Logistic Rregession ROC AUC Score:\", roc_lr)\n",
        "\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "\n",
        "# model prediction\n",
        "pred_gnb = model_gnb.predict(X_gnb_test)\n",
        "\n",
        "# evaluation\n",
        "bal_acc_gnb = balanced_accuracy_score(Y_gnb_test, pred_gnb) #evaluating balanced accuracy (Scikit, 2025)\n",
        "roc_gnb = roc_auc_score(Y_gnb_test, model_gnb.predict_proba(X_gnb_test), multi_class=\"ovr\", average=\"macro\") #calculating ROC score, using one-vs rest approach to calculate one class against all other classes (Scikit, 2025)\n",
        "\n",
        "print(\"\\nGNB Balanced Accuracy:\", bal_acc_gnb)\n",
        "print(\"GNB AOC AUC Score:\", roc_gnb)\n",
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "\n",
        "# Predictions\n",
        "pred_svm = model_svm.predict(X_svm_test)\n",
        "y_prob = model_svm.predict_proba(X_svm_test)\n",
        "\n",
        "# Evaluations\n",
        "bal_acc_svm = metrics.balanced_accuracy_score(Y_svm_test, pred_svm) #(Scikit,2025)\n",
        "roc_svm = metrics.roc_auc_score(Y_svm_test, y_prob, multi_class=\"ovr\", average=\"macro\") #(Scikit,2025)\n",
        "\n",
        "print(\"\\nSVM Balanced Accuracy:\", bal_acc_svm)\n",
        "print(\"SVM ROC AUC Score:\", roc_svm)\n",
        "\n",
        "\n",
        "# K-nearest neighbors (KNN)\n",
        "\n",
        "# Predictions\n",
        "pred_knn = model_knn.predict(X_knn_test)\n",
        "y_prob = model_knn.predict_proba(X_knn_test)\n",
        "\n",
        "# Evaluations\n",
        "bal_acc_knn = balanced_accuracy_score(Y_knn_test, pred_knn) #(Scikit,2025)\n",
        "roc_knn = roc_auc_score(Y_knn_test, y_prob, multi_class=\"ovr\", average=\"macro\") #(Scikit,2025)\n",
        "\n",
        "print(f\"\\nKNN Balanced Accuracy: {bal_acc_knn:.4f}\")\n",
        "print(f\"KNN ROC-AUC Score: {roc_knn:.4f}\")\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "# Predictions\n",
        "pred_rf = model_rf.predict(X_rf_test)\n",
        "y_prob = model_rf.predict_proba(X_rf_test)\n",
        "\n",
        "# Evaluations\n",
        "bal_acc_rf =balanced_accuracy_score(Y_rf_test, pred_rf)\n",
        "roc_rf = roc_auc_score(Y_rf_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
        "\n",
        "print(\"\\nRandom Forest Balanced Accuracy:\", bal_acc_rf)\n",
        "print(\"Random Forest ROC AUC Score:\", roc_rf)\n",
        "\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "\n",
        "# Predictions\n",
        "pred_qda = model_qda.predict(X_qda_test)\n",
        "y_prob_qda = model_qda.predict_proba(X_qda_test)\n",
        "\n",
        "# Evaluations\n",
        "bal_acc_qda = balanced_accuracy_score(Y_qda_test, pred_qda)\n",
        "roc_qda = roc_auc_score(Y_qda_test, y_prob_qda, multi_class=\"ovr\", average=\"macro\")\n",
        "\n",
        "print(f\"\\nQDA Balanced Accuracy: {bal_acc_qda:.4f}\")\n",
        "print(f\"QDA ROC-AUC Score: {roc_qda:.4f}\")\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# Confusion Matrices\n",
        "\n",
        "# Confusion Matrix for Decision Tree\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(Y_dt_test, pred_dt), annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Decision Tree\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion Matrix for LDA\n",
        "cm_lda = confusion_matrix(Y_lda_test, pred_lda)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_lda, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for LDA\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion matrix for Logistic Regression\n",
        "cm_lr = confusion_matrix(Y_lr_test, pred_lr)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_lr, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion matrix for GNB\n",
        "cm_gnb = confusion_matrix(Y_gnb_test, pred_gnb)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_gnb, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for GNB\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion Matrix for SVM\n",
        "cm_svm = confusion_matrix(Y_svm_test, pred_svm)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_svm, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for SVM\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion Matrix for KNN\n",
        "cm_knn = confusion_matrix(Y_knn_test, pred_knn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_knn, annot=True, fmt=\"d\",  cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for KNN\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#confusion matrix for Random Forest\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(Y_rf_test, pred_rf), annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Random Forest\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion Matrix for QDA\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(Y_qda_test, pred_qda), annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for QDA\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeJYyb053diy"
      },
      "source": [
        "##Hyperparamter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpjfoOd3pEP"
      },
      "source": [
        "Finding optimal parameters for each model and training them subsequently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-PLCmfBuhNJ",
        "outputId": "a37e7e3b-65a2-42a6-d4b9-ac797c05769f"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "\n",
        "#hyperparameter tuning using gridsearchcv  (Machine Learning Mastery, 2020) (Brownlee, J 2020)\n",
        "print(\"Optimal Parameters for Decision Tree\")\n",
        "param_grid_dr = {\n",
        "    'max_depth': [None, 10, 20, 30, 40],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_features': [None, 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# perform GridSearchCV\n",
        "grid_search_dr = GridSearchCV(DecisionTreeClassifier(random_state=72), param_grid_dr, cv=5, scoring=\"balanced_accuracy\", verbose=1)\n",
        "grid_search_dr.fit(X_dt_train, Y_dt_train)\n",
        "\n",
        "# Best Parameters\n",
        "best_dt = grid_search_dr.best_estimator_\n",
        "print(f\"Parameters: {grid_search_dr.best_params_}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train optimized Decision Tree model with best parameters\n",
        "optimal_model_dr = DecisionTreeClassifier(**grid_search_dr.best_params_, random_state=72)\n",
        "optimal_model_dr.fit(X_dt_train, Y_dt_train)\n",
        "\n",
        "\n",
        "# Linear discriminant analysis\n",
        "\n",
        "\n",
        "# hyperparameter tuning using GridSearchCV # (Kaggle,2019) (Hern√°ndez, J. 2019)\n",
        "print(\"Optimal Parameters for LDA\")\n",
        "param_grid = [\n",
        "    {\"solver\": [\"svd\"]},\n",
        "    {\"solver\": [\"lsqr\", \"eigen\"], \"shrinkage\": [\"auto\"]}  # Including shrinkage parameter\n",
        "]\n",
        "\n",
        "# perform GridSearchCV for LDA model\n",
        "grid_search_lda = GridSearchCV(LinearDiscriminantAnalysis(), param_grid, cv=5, scoring=\"balanced_accuracy\", verbose=1)\n",
        "grid_search_lda.fit(X__lda_train, Y_lda_train)\n",
        "\n",
        "# best parameters\n",
        "print(f\"Parameters: {grid_search_lda.best_params_}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train the optimized LDA model\n",
        "best_lda = grid_search_lda.best_estimator_\n",
        "best_lda.fit(X__lda_train, Y_lda_train)\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "\n",
        "# Hypertuning parameters using GridSearchCV (Scikit,2025)(Geron, 2017)\n",
        "print(\"Optimal Parameters for Logistic Regression\")\n",
        "param_grid = {\n",
        "    \"C\": [1, 10, 100],\n",
        "    \"penalty\": [\"l1\", \"l2\"], #Certain hyperparameters, such as elasticnet and none for the penalty parameter, were excluded from the tuning process due to their incompatibility with specific solvers in the logistic regression implementation. Additionally, these parameters were omitted to optimize computational efficiency, as their inclusion would significantly increase the computational complexity and runtime of the hyperparameter search process while not contributing to accuracy improvement\n",
        "    \"solver\": [\"saga\"], #solver compatible with l1 and l2\n",
        "    \"max_iter\": [10000] #Setting a high number to allow the algorithm enough iterations to give optimal sets of parameters\n",
        "\n",
        "}\n",
        "\n",
        "grid_search_lr = GridSearchCV(model_lr, param_grid= param_grid, cv=5, verbose=True)\n",
        "grid_search_lr.fit(X_lr_train, Y_lr_train)\n",
        "\n",
        "print(\"Parameters:\", grid_search_lr.best_params_)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train the optimised logistic Regression\n",
        "optimal_model_lr = grid_search_lr.best_estimator_\n",
        "optimal_model_lr.fit(X_lr_train, Y_lr_train)\n",
        "\n",
        "\n",
        "# Gaussian Niave Bayes\n",
        "\n",
        "\n",
        "# Hyperparamter tuning to find optimal parameters (Scikit, 2025)\n",
        "print(\"Optimal Parameters for Gaussian Naive Bayes\")\n",
        "param_grid = {\"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6, 1e-5], #to increase stability\n",
        "              \"priors\": [None],} #since the dataset is appromximately balanced, I opted to not specify prior probabilities as its almost equal for each class. Aditionally, specifiying prior probabilites yields the same result as deafualt priors obtained from training the data. This suggests that the parameter is already optimal and does not need to be tuned\n",
        "\n",
        "grid_search = GridSearchCV(model_gnb, param_grid, cv=5, verbose=True)\n",
        "grid_search.fit(X_gnb_train, Y_gnb_train)\n",
        "print(\"Parameters: \", grid_search.best_params_)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train the optimised Gaussian Niave bayes\n",
        "optimal_model_gnb = grid_search.best_estimator_\n",
        "optimal_model_gnb.fit(X_gnb_train, Y_gnb_train)\n",
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "print(\"Optimal Parameters for Support Vector Machine\")\n",
        "\n",
        "param_grid = {\n",
        "    \"C\": [0.1, 1, 10, 100],  # Regularization strength\n",
        "    \"gamma\": [0.01, 0.001, 0.0001],  # Kernel coefficient\n",
        "    \"kernel\": [\"rbf\"]  # Using Radial Basis Function (RBF) kernel\n",
        "}\n",
        "\n",
        "# Perform GridSearch (Scikit,2025)\n",
        "grid_search_svm = GridSearchCV(svm.SVC(probability=True), param_grid, cv=5, scoring=\"accuracy\", verbose=1)\n",
        "grid_search_svm.fit(X_svm_train, Y_svm_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best Parameters: {grid_search_svm.best_params_}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# K-nearest neighbors (KNN)\n",
        "print(\"Optimal Parameters for KNN - K-Nearest neighbours\")\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    \"n_neighbors\": [3, 5, 7, 9, 11],  # Number of neighbors\n",
        "    \"weights\": [\"uniform\", \"distance\"],  # Voting strategy\n",
        "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]  # Distance metric\n",
        "}\n",
        "\n",
        "# Perform GridSearch\n",
        "grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring=\"balanced_accuracy\", verbose=1) #(Medium,2023) (Agrawal, S. 2023)\n",
        "grid_search_knn.fit(X_knn_train, Y_knn_train)\n",
        "\n",
        "# Best Parameters\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "print(f\"Best KNN Parameters: {grid_search_knn.best_params_}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "print(\"Optimal Parameters for Random Forest\")\n",
        "\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100],                 \n",
        "    'max_depth': [None, 10],              \n",
        "    'min_samples_split': [2],              \n",
        "    'min_samples_leaf': [1],               \n",
        "    'criterion': ['gini'],                \n",
        "    'max_features': [None],    \n",
        "    'bootstrap': [True]                        \n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=72), param_grid_rf, cv=5, scoring=\"balanced_accuracy\", verbose=1)\n",
        "grid_search_rf.fit(X_rf_train, Y_rf_train)\n",
        "\n",
        "# Best Parameters\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "print(f\"Best Random Forest Parameters: {grid_search_rf.best_params_}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "print(\"Optimal Parameters for Quadratic Discriminant Analysis\")\n",
        "\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    \"reg_param\": [0.1, 0.5, 1.0],  # Testing different regularization values\n",
        "    \"tol\": [1e-4, 1e-3, 1e-2]  # Testing different tolerance values\n",
        "}\n",
        "\n",
        "# GridSearchCV to tune QDA parameters with higher verbosity\n",
        "grid_search_qda = GridSearchCV(QuadraticDiscriminantAnalysis(), param_grid, cv=5, scoring=\"balanced_accuracy\", verbose=1)\n",
        "grid_search_qda.fit(X_qda_train, Y_qda_train)\n",
        "\n",
        "# Best parameters found by GridSearchCV\n",
        "print(f\"Best QDA Parameters: {grid_search_qda.best_params_}\")\n",
        "\n",
        "##LAST SENTENCE\n",
        "print (\"All models are trained with optimal parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn2JyPGu4qLZ"
      },
      "source": [
        "##Optimal Model Evaluation (code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91uLUgW34747"
      },
      "source": [
        "Evaluating each model with optimal set of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MgLfUILtNlv-",
        "outputId": "8d66f5ef-28a8-4cb7-ad12-2c3577519898"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Decision Tree\n",
        "\n",
        "# Predictions and performance evaluation\n",
        "optimal_pred_dr = optimal_model_dr.predict(X_dt_test)\n",
        "optimal_prob_dr = optimal_model_dr.predict_proba(X_dt_test)\n",
        "\n",
        "print(f\"Optimised Decision Tree Balanced Accuracy: {balanced_accuracy_score(Y_dt_test, optimal_pred_dr):.4f}\")\n",
        "print(f\"Optimised Decision Tree ROC-AUC Score: {roc_auc_score(Y_dt_test, optimal_prob_dr, multi_class='ovr', average='macro'):.4f}\")\n",
        "\n",
        "\n",
        "# Linear discriminant analysis\n",
        "\n",
        "# predictions\n",
        "optimal_pred_lda = best_lda.predict(X_lda_test)\n",
        "optimal_prob_lda = best_lda.predict_proba(X_lda_test)\n",
        "\n",
        "# evaluations\n",
        "bal_acc_lda = balanced_accuracy_score(Y_lda_test, optimal_pred_lda)\n",
        "roc_lda = roc_auc_score(Y_lda_bin_test, optimal_prob_lda, multi_class='ovr',average=\"macro\")\n",
        "\n",
        "print(f\"\\nOptimised LDA Balanced Accuracy: {bal_acc_lda:.4f}\")\n",
        "print(f\"Optimised LDA ROC-AUC Score: {roc_lda:.4f}\")\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "# Predicting classes for digits dataset with optimal set of parameters\n",
        "optimal_pred_lr = optimal_model_lr.predict(X_lr_test)\n",
        "optimal_prob_lr = optimal_model_lr.predict_proba(X_lr_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nOptimised Logistic Regression Balanced Accuracy:\", balanced_accuracy_score(Y_lr_test, optimal_pred_lr))\n",
        "roc_optmial_model_lr = roc_auc_score(Y_lr_test, optimal_model_lr.predict_proba(X_lr_test), multi_class=\"ovr\", average=\"macro\")\n",
        "print(\"Optmised Logistic Regression ROC-AUC Score:\", roc_optmial_model_lr)\n",
        "\n",
        "\n",
        "# Gaussian Niave Bayes\n",
        "\n",
        "# Predicting classes for digits dataset with optimal set of parameters\n",
        "optimal_pred_gnb = optimal_model_gnb.predict(X_gnb_test)\n",
        "optimal_prob_gnb = optimal_model_gnb.predict_proba(X_gnb_test)\n",
        "\n",
        "# Evaluation\n",
        "bal_acc_best_model_gnb = balanced_accuracy_score(Y_gnb_test, optimal_pred_gnb)\n",
        "print(\"\\nOptimal GNB Balanced Accuracy:\", bal_acc_best_model_gnb)\n",
        "roc_optmial_model_gnb = roc_auc_score(Y_gnb_test, optimal_model_gnb.predict_proba(X_gnb_test), multi_class=\"ovr\", average=\"macro\")\n",
        "print(\"Optimal GNB ROC-AUC Score:\", roc_optmial_model_gnb)\n",
        "\n",
        "\n",
        "# K-nearest neighbors (KNN)\n",
        "\n",
        "# Get the best parameters from GridSearchCV\n",
        "best_k = grid_search_knn.best_params_[\"n_neighbors\"]  \n",
        "best_weights = grid_search_knn.best_params_[\"weights\"]\n",
        "best_metric = grid_search_knn.best_params_[\"metric\"]\n",
        "\n",
        "optimal_model_knn = KNeighborsClassifier(n_neighbors=best_k, weights=best_weights, metric=best_metric)\n",
        "optimal_model_knn.fit(X_knn_train, Y_knn_train)\n",
        "\n",
        "# Predictions\n",
        "optimal_pred_knn = optimal_model_knn.predict(X_knn_test)\n",
        "optimal_prob_knn = optimal_model_knn.predict_proba(X_knn_test)\n",
        "\n",
        "# Evaluate performance\n",
        "bal_acc_optimized = accuracy_score(Y_knn_test, optimal_pred_knn)\n",
        "roc_auc_optimized = roc_auc_score(Y_knn_test, optimal_prob_knn, multi_class=\"ovr\")\n",
        "\n",
        "print(f\"Optimized KNN Balanced Accuracy: {bal_acc_optimized:.4f}\")\n",
        "print(f\"Optimized KNN ROC-AUC Score: {roc_auc_optimized:.4f}\")\n",
        "\n",
        "# Support Vector Machine\n",
        "\n",
        "best_C = grid_search_svm.best_params_[\"C\"] # Choosing the best C from after running the hyperparameter tuning\n",
        "best_gamma = grid_search_svm.best_params_[\"gamma\"]  # Choosing the best gamma from after running the hyperparameter tuning \n",
        "\n",
        "# Train optimized SVM model\n",
        "optimal_model_svm = svm.SVC(C=best_C, gamma=best_gamma, kernel=\"rbf\", probability=True)\n",
        "optimal_model_svm.fit(X_svm_train, Y_svm_train)\n",
        "\n",
        "# Predictions\n",
        "optimal_pred_svm = optimal_model_svm.predict(X_svm_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_optimized = metrics.accuracy_score(Y_svm_test, optimal_pred_svm)\n",
        "print(f\"Optimized SVM Accuracy: {accuracy_optimized:.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "\n",
        "# Extract best parameters from GridSearch\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "\n",
        "# Train optimized Random Forest model\n",
        "optimal_model_rf = RandomForestClassifier(**best_params_rf, random_state=72)\n",
        "\n",
        "# Fit the optimized model\n",
        "optimal_model_rf.fit(X_rf_train, Y_rf_train)\n",
        "\n",
        "# Predictions & evaluation\n",
        "optimal_pred_rf = optimal_model_rf.predict(X_rf_test)\n",
        "accuracy_optimized_rf = accuracy_score(Y_rf_test, optimal_pred_rf)\n",
        "bal_acc_rf = balanced_accuracy_score(Y_rf_test, optimal_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimized Random Forest Accuracy: {accuracy_optimized_rf:.4f}\")\n",
        "print(f\"Balanced Accuracy: {bal_acc_rf:.4f}\")\n",
        "\n",
        "# Quadratic Discriminant Analysis\n",
        "\n",
        "# best parameters\n",
        "best_reg_param = grid_search_qda.best_params_[\"reg_param\"]\n",
        "best_tol = grid_search_qda.best_params_[\"tol\"]\n",
        "\n",
        "# Train optimized QDA model \n",
        "optimal_model_qda = QuadraticDiscriminantAnalysis(reg_param=best_reg_param, tol=best_tol)\n",
        "optimal_model_qda.fit(X_qda_train, Y_qda_train)\n",
        "\n",
        "# Predictions\n",
        "optimal_pred_qda = optimal_model_qda.predict(X_qda_test)\n",
        "optimal_prob_qda = optimal_model_qda.predict_proba(X_qda_test)\n",
        "\n",
        "# Evaluate performance\n",
        "bal_acc_optimized_qda = accuracy_score(Y_qda_test, optimal_pred_qda)\n",
        "roc_auc_optimized_qda = roc_auc_score(Y_qda_test, optimal_prob_qda, multi_class=\"ovr\")\n",
        "\n",
        "print(f\"Optimized QDA Balanced Accuracy: {bal_acc_optimized_qda:.4f}\")\n",
        "print(f\"Optimized QDA ROC-AUC Score: {roc_auc_optimized_qda:.4f}\")\n",
        "\n",
        "\n",
        "# Confusion Matrices for Optimised Models\n",
        "\n",
        "# Confusion Matrix for optimised Decision tree\n",
        "cm_optimal_model_dt = confusion_matrix(Y_dt_test, optimal_pred_dr)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_model_dt, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimised Decision Tree\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Confusion Matrix for optimised LDA\n",
        "cm_optimal_model_lda = confusion_matrix(Y_lda_test, optimal_pred_lda)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_model_lda, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimised LDA\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Confusion matrix for optimised Logistic Regression\n",
        "cm_optimal_model_lr = confusion_matrix(Y_lr_test, optimal_pred_lr)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_optimal_model_lr, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimised Logistic Regression\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Confusion matrix for optimsed GNB\n",
        "cm_optimal_model_gnb = confusion_matrix(Y_gnb_test, optimal_pred_gnb)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_optimal_model_gnb, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimised GNB\")\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "#Confusion matrix for optimised Knn\n",
        "cm_optimal_knn = confusion_matrix(Y_knn_test, optimal_pred_knn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_knn, annot=True, fmt=\"d\",  cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Default KNN\")\n",
        "plt.show()\n",
        "\n",
        "#Confusion matrix for optimised SVM\n",
        "\n",
        "cm_optimal_svm = confusion_matrix(Y_svm_test, optimal_pred_svm)\n",
        "\n",
        "#ploting the matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_svm, annot=True, fmt=\"d\",  cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Default KNN\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Confusion Matrix for the optimized QDA model\n",
        "cm_optimal_qda = confusion_matrix(Y_qda_test, optimal_pred_qda)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_qda, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimized QDA\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for the optimized Random Forest model\n",
        "\n",
        "cm_optimal_rf = confusion_matrix(Y_rf_test, optimal_pred_qda)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_optimal_rf, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for Optimized Random Forest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC curve\n",
        "\n",
        "print (\"All Models are trained\")\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": (model_dt, X_dt_test, Y_dt_test),\n",
        "    \"LDA\": (model_lda, X_lda_test, Y_lda_test),\n",
        "    \"Logistic Regression\": (model_lr, X_lr_test, Y_lr_test),\n",
        "    \"Gaussian Naive Bayes\": (model_gnb, X_gnb_test, Y_gnb_test),\n",
        "    \"SVM\": (model_svm, X_svm_test, Y_svm_test),\n",
        "    \"KNN\": (model_knn, X_knn_test, Y_knn_test),\n",
        "    \"Random Forest\": (model_rf, X_rf_test, Y_rf_test),\n",
        "    \"QDA\": (model_qda, X_qda_test, Y_qda_test),\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "n_classes = len(np.unique(Y_dt_test))\n",
        "\n",
        "for name, (model, X_test, Y_test) in models.items():\n",
        "    Y_test_bin = label_binarize(Y_test, classes=np.unique(Y_test))  \n",
        "    \n",
        "    y_score = model.predict_proba(X_test)\n",
        "    \n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    mean_tpr /= n_classes\n",
        "    macro_auc = auc(all_fpr, mean_tpr)\n",
        "    \n",
        "    plt.plot(all_fpr, mean_tpr, label=f\"{name} (AUC = {macro_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Macro-Averaged ROC Curves for Classification Models\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNSKVG-34Be4"
      },
      "source": [
        "##References (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7L_OTsr4Gdw"
      },
      "source": [
        "List any references you may have used in your document before, using one of the established referencing system (e.g. IEEE, Harvard, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn-sDYIX4DIf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
